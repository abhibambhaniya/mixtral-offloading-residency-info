{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 13 20:42:28 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:C7:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             119W / 700W |  12103MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   3307446      C   /scratch/584564/moe_offload/bin/python    12074MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://people.eecs.berkeley.edu/~hendrycks/data.tar -O mmlu.tar\n",
    "# !tar -xf mmlu.tar -C mmlu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will reload the imported modules (e.g. get_decode_model_characterstics) every time you execute the jupyter cells, so that you don't need to restart the notebook after updating the source codes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mhqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/584564/moe_offload/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/scratch/584564/moe_offload/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys\n",
    "script_dir = os.getcwd()\n",
    "module_path = script_dir\n",
    "for _ in range(1):\n",
    "    module_path = os.path.abspath(os.path.join(module_path, '../'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.insert(0,module_path)\n",
    "        \n",
    "sys.path.append(\"mixtral-offloading\")\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from hqq.core.quantize import BaseQuantizeConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import trange\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers.utils import logging as hf_logging\n",
    "import time\n",
    "import gc\n",
    "from src.build_model import OffloadConfig, QuantConfig, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(quantized_model_name)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n",
    "offload_per_layer = 4\n",
    "# offload_per_layer = 5\n",
    "###############################################################\n",
    "\n",
    "num_experts = config.num_local_experts\n",
    "\n",
    "offload_config = OffloadConfig(\n",
    "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
    "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
    "    buffer_size=4,\n",
    "    offload_per_layer=offload_per_layer,\n",
    ")\n",
    "\n",
    "\n",
    "attn_config = BaseQuantizeConfig(\n",
    "    nbits=4,\n",
    "    group_size=64,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
    "\n",
    "\n",
    "ffn_config = BaseQuantizeConfig(\n",
    "    nbits=2,\n",
    "    group_size=16,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
    "\n",
    "\n",
    "# del model\n",
    "\n",
    "gc.collect\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/584564/moe_offload/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Loading experts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  7.15it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_model(\n",
    "    device=device,\n",
    "    quant_config=quant_config,\n",
    "    offload_config=offload_config,\n",
    "    state_path=state_path,\n",
    "    routing_strategy=\"THRESHOLDING\",\n",
    "    routing_threshold=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting abstract_algebra, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.300 , Average Time:1.161 sec- 592.8, avg expert load reduced:abstract_algebra\n",
      "Starting anatomy, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.700 , Average Time:1.176 sec- 678.6, avg expert load reduced:anatomy\n",
      "Starting astronomy, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.700 , Average Time:1.416 sec- 762.4, avg expert load reduced:astronomy\n",
      "Starting business_ethics, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.700 , Average Time:1.382 sec- 840.2, avg expert load reduced:business_ethics\n",
      "Starting clinical_knowledge, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.700 , Average Time:1.249 sec- 911.8, avg expert load reduced:clinical_knowledge\n",
      "Starting college_biology, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.900 , Average Time:1.316 sec- 985.0, avg expert load reduced:college_biology\n",
      "Starting college_chemistry, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.500 , Average Time:1.397 sec- 1055.9, avg expert load reduced:college_chemistry\n",
      "Starting college_computer_science, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.500 , Average Time:1.645 sec- 1129.7, avg expert load reduced:college_computer_science\n",
      "Starting college_mathematics, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.400 , Average Time:1.446 sec- 1193.4, avg expert load reduced:college_mathematics\n",
      "Starting college_medicine, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.600 , Average Time:1.340 sec- 1268.1, avg expert load reduced:college_medicine\n",
      "Starting college_physics, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.600 , Average Time:1.337 sec- 1347.7, avg expert load reduced:college_physics\n",
      "Starting computer_security, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.600 , Average Time:1.240 sec- 1430.0, avg expert load reduced:computer_security\n",
      "Starting conceptual_physics, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.800 , Average Time:1.125 sec- 1504.3, avg expert load reduced:conceptual_physics\n",
      "Starting econometrics, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.600 , Average Time:1.468 sec- 1579.4, avg expert load reduced:econometrics\n",
      "Starting electrical_engineering, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.700 , Average Time:1.289 sec- 1651.4, avg expert load reduced:electrical_engineering\n",
      "Starting elementary_mathematics, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.400 , Average Time:1.384 sec- 1723.3, avg expert load reduced:elementary_mathematics\n",
      "Starting formal_logic, dev size:(5, 6), Test size:(10, 6)\n",
      "Average accuracy 0.700 , Average Time:1.487 sec- 1794.3, avg expert load reduced:formal_logic\n",
      "Starting global_facts, dev size:(5, 6), Test size:(10, 6)\n"
     ]
    }
   ],
   "source": [
    "from mmlu import test_mmlu\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "test_mmlu(model_name=model_name, model_loaded=model, tokenizer=tokenizer, data_dir=\"./mmlu\", save_dir=\"./thresholding_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result Ref: https://wandb.ai/byyoung3/ml-news/reports/Testing-Mixtral-8x7B-with-MMLU-and-W-B---Vmlldzo2MjI0ODAz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
