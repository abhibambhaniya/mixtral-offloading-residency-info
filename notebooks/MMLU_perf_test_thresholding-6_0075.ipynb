{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 16 17:49:22 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          Off | 00000000:BE:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              73W / 700W |      4MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://people.eecs.berkeley.edu/~hendrycks/data.tar -O mmlu.tar\n",
    "# !tar -xf mmlu.tar -C mmlu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will reload the imported modules (e.g. get_decode_model_characterstics) every time you execute the jupyter cells, so that you don't need to restart the notebook after updating the source codes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mhqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/593708/moe_offload/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys\n",
    "script_dir = os.getcwd()\n",
    "module_path = script_dir\n",
    "for _ in range(1):\n",
    "    module_path = os.path.abspath(os.path.join(module_path, '../'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.insert(0,module_path)\n",
    "        \n",
    "sys.path.append(\"mixtral-offloading\")\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from hqq.core.quantize import BaseQuantizeConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import trange\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers.utils import logging as hf_logging\n",
    "import time\n",
    "import gc\n",
    "from src.build_model import OffloadConfig, QuantConfig, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo2\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(quantized_model_name)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n",
    "offload_per_layer = 6\n",
    "# offload_per_layer = 5\n",
    "###############################################################\n",
    "\n",
    "num_experts = config.num_local_experts\n",
    "\n",
    "offload_config = OffloadConfig(\n",
    "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
    "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
    "    buffer_size=4,\n",
    "    offload_per_layer=offload_per_layer,\n",
    ")\n",
    "\n",
    "\n",
    "attn_config = BaseQuantizeConfig(\n",
    "    nbits=4,\n",
    "    group_size=64,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
    "\n",
    "\n",
    "ffn_config = BaseQuantizeConfig(\n",
    "    nbits=2,\n",
    "    group_size=16,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
    "\n",
    "\n",
    "# del model\n",
    "\n",
    "gc.collect\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/593708/moe_offload/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Loading experts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  6.55it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_model(\n",
    "    device=device,\n",
    "    quant_config=quant_config,\n",
    "    offload_config=offload_config,\n",
    "    state_path=state_path,\n",
    "    routing_strategy=\"THRESHOLDING\",\n",
    "    routing_threshold=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting abstract_algebra, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.340 , Average Time:1.310 sec, avg expert load reduced: 1132.55, - abstract_algebra\n",
      "Starting anatomy, dev size:(5, 6), Test size:(135, 6)\n",
      "Average accuracy 0.593 , Average Time:1.307 sec, avg expert load reduced: 3639.5703703703703, - anatomy\n",
      "Starting astronomy, dev size:(5, 6), Test size:(152, 6)\n",
      "Average accuracy 0.789 , Average Time:1.552 sec, avg expert load reduced: 6559.664473684211, - astronomy\n",
      "Starting business_ethics, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.670 , Average Time:1.531 sec, avg expert load reduced: 9094.06, - business_ethics\n",
      "Starting clinical_knowledge, dev size:(5, 6), Test size:(265, 6)\n",
      "Average accuracy 0.758 , Average Time:1.383 sec, avg expert load reduced: 12676.781132075472, - clinical_knowledge\n",
      "Starting college_biology, dev size:(5, 6), Test size:(144, 6)\n",
      "Average accuracy 0.764 , Average Time:1.444 sec, avg expert load reduced: 16781.729166666668, - college_biology\n",
      "Starting college_chemistry, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.480 , Average Time:1.511 sec, avg expert load reduced: 19275.49, - college_chemistry\n",
      "Starting college_computer_science, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.540 , Average Time:1.760 sec, avg expert load reduced: 21324.54, - college_computer_science\n",
      "Starting college_mathematics, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.390 , Average Time:1.541 sec, avg expert load reduced: 23343.68, - college_mathematics\n",
      "Starting college_medicine, dev size:(5, 6), Test size:(173, 6)\n",
      "Average accuracy 0.665 , Average Time:1.482 sec, avg expert load reduced: 26114.323699421966, - college_medicine\n",
      "Starting college_physics, dev size:(5, 6), Test size:(102, 6)\n",
      "Average accuracy 0.480 , Average Time:1.442 sec, avg expert load reduced: 28907.745098039217, - college_physics\n",
      "Starting computer_security, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.720 , Average Time:1.322 sec, avg expert load reduced: 30958.13, - computer_security\n",
      "Starting conceptual_physics, dev size:(5, 6), Test size:(235, 6)\n",
      "Average accuracy 0.643 , Average Time:1.238 sec, avg expert load reduced: 34452.29361702128, - conceptual_physics\n",
      "Starting econometrics, dev size:(5, 6), Test size:(114, 6)\n",
      "Average accuracy 0.456 , Average Time:1.571 sec, avg expert load reduced: 38054.44736842105, - econometrics\n",
      "Starting electrical_engineering, dev size:(5, 6), Test size:(145, 6)\n",
      "Average accuracy 0.593 , Average Time:1.388 sec, avg expert load reduced: 40704.68275862069, - electrical_engineering\n",
      "Starting elementary_mathematics, dev size:(5, 6), Test size:(378, 6)\n",
      "Average accuracy 0.458 , Average Time:1.503 sec, avg expert load reduced: 45957.08201058201, - elementary_mathematics\n",
      "Starting formal_logic, dev size:(5, 6), Test size:(126, 6)\n",
      "Average accuracy 0.516 , Average Time:1.590 sec, avg expert load reduced: 51052.793650793654, - formal_logic\n",
      "Starting global_facts, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.370 , Average Time:1.372 sec, avg expert load reduced: 53353.23, - global_facts\n",
      "Starting high_school_biology, dev size:(5, 6), Test size:(310, 6)\n",
      "Average accuracy 0.819 , Average Time:1.482 sec, avg expert load reduced: 57485.254838709676, - high_school_biology\n",
      "Starting high_school_chemistry, dev size:(5, 6), Test size:(203, 6)\n",
      "Average accuracy 0.517 , Average Time:1.466 sec, avg expert load reduced: 62682.487684729065, - high_school_chemistry\n",
      "Starting high_school_computer_science, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.770 , Average Time:1.837 sec, avg expert load reduced: 65692.12, - high_school_computer_science\n",
      "Starting high_school_european_history, dev size:(5, 6), Test size:(165, 6)\n",
      "Average accuracy 0.758 , Average Time:2.624 sec, avg expert load reduced: 68275.3393939394, - high_school_european_history\n",
      "Starting high_school_geography, dev size:(5, 6), Test size:(198, 6)\n",
      "Average accuracy 0.813 , Average Time:1.318 sec, avg expert load reduced: 71959.12626262626, - high_school_geography\n",
      "Starting high_school_government_and_politics, dev size:(5, 6), Test size:(193, 6)\n",
      "Average accuracy 0.933 , Average Time:1.406 sec, avg expert load reduced: 75944.1865284974, - high_school_government_and_politics\n",
      "Starting high_school_macroeconomics, dev size:(5, 6), Test size:(390, 6)\n",
      "Average accuracy 0.672 , Average Time:1.314 sec, avg expert load reduced: 81788.04871794872, - high_school_macroeconomics\n",
      "Starting high_school_mathematics, dev size:(5, 6), Test size:(270, 6)\n",
      "Average accuracy 0.315 , Average Time:1.477 sec, avg expert load reduced: 88263.31851851851, - high_school_mathematics\n",
      "Starting high_school_microeconomics, dev size:(5, 6), Test size:(238, 6)\n",
      "Average accuracy 0.748 , Average Time:1.338 sec, avg expert load reduced: 93215.50420168067, - high_school_microeconomics\n",
      "Starting high_school_physics, dev size:(5, 6), Test size:(151, 6)\n",
      "Average accuracy 0.411 , Average Time:1.507 sec, avg expert load reduced: 97098.64900662252, - high_school_physics\n",
      "Starting high_school_psychology, dev size:(5, 6), Test size:(545, 6)\n",
      "Average accuracy 0.855 , Average Time:1.448 sec, avg expert load reduced: 104107.64587155964, - high_school_psychology\n",
      "Starting high_school_statistics, dev size:(5, 6), Test size:(216, 6)\n",
      "Average accuracy 0.579 , Average Time:1.767 sec, avg expert load reduced: 111775.42129629629, - high_school_statistics\n",
      "Starting high_school_us_history, dev size:(5, 6), Test size:(204, 6)\n",
      "Average accuracy 0.828 , Average Time:2.650 sec, avg expert load reduced: 115916.50980392157, - high_school_us_history\n",
      "Starting high_school_world_history, dev size:(5, 6), Test size:(237, 6)\n",
      "Average accuracy 0.819 , Average Time:2.397 sec, avg expert load reduced: 120163.88607594937, - high_school_world_history\n",
      "Starting human_aging, dev size:(5, 6), Test size:(223, 6)\n",
      "Average accuracy 0.700 , Average Time:1.252 sec, avg expert load reduced: 124676.7399103139, - human_aging\n",
      "Starting human_sexuality, dev size:(5, 6), Test size:(131, 6)\n",
      "Average accuracy 0.748 , Average Time:1.291 sec, avg expert load reduced: 128244.60305343512, - human_sexuality\n",
      "Starting international_law, dev size:(5, 6), Test size:(121, 6)\n",
      "Average accuracy 0.818 , Average Time:1.592 sec, avg expert load reduced: 130826.57851239669, - international_law\n",
      "Starting jurisprudence, dev size:(5, 6), Test size:(108, 6)\n",
      "Average accuracy 0.769 , Average Time:1.340 sec, avg expert load reduced: 133177.0462962963, - jurisprudence\n",
      "Starting logical_fallacies, dev size:(5, 6), Test size:(163, 6)\n",
      "Average accuracy 0.761 , Average Time:1.375 sec, avg expert load reduced: 135916.98159509202, - logical_fallacies\n",
      "Starting machine_learning, dev size:(5, 6), Test size:(112, 6)\n",
      "Average accuracy 0.518 , Average Time:1.600 sec, avg expert load reduced: 138709.4107142857, - machine_learning\n",
      "Starting management, dev size:(5, 6), Test size:(103, 6)\n",
      "Average accuracy 0.786 , Average Time:1.216 sec, avg expert load reduced: 140935.36893203884, - management\n",
      "Starting marketing, dev size:(5, 6), Test size:(234, 6)\n",
      "Average accuracy 0.893 , Average Time:1.352 sec, avg expert load reduced: 144353.29487179487, - marketing\n",
      "Starting medical_genetics, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.750 , Average Time:1.295 sec, avg expert load reduced: 147757.92, - medical_genetics\n",
      "Starting miscellaneous, dev size:(5, 6), Test size:(783, 6)\n",
      "Average accuracy 0.845 , Average Time:1.249 sec, avg expert load reduced: 156572.04980842912, - miscellaneous\n",
      "Starting moral_disputes, dev size:(5, 6), Test size:(346, 6)\n",
      "Average accuracy 0.728 , Average Time:1.427 sec, avg expert load reduced: 167823.45375722542, - moral_disputes\n",
      "Starting moral_scenarios, dev size:(5, 6), Test size:(895, 6)\n",
      "Average accuracy 0.412 , Average Time:1.609 sec, avg expert load reduced: 181464.89162011174, - moral_scenarios\n",
      "Starting nutrition, dev size:(5, 6), Test size:(306, 6)\n",
      "Average accuracy 0.771 , Average Time:1.560 sec, avg expert load reduced: 194559.78758169935, - nutrition\n",
      "Starting philosophy, dev size:(5, 6), Test size:(311, 6)\n",
      "Average accuracy 0.727 , Average Time:1.272 sec, avg expert load reduced: 200806.33762057876, - philosophy\n",
      "Starting prehistory, dev size:(5, 6), Test size:(324, 6)\n",
      "Average accuracy 0.759 , Average Time:1.489 sec, avg expert load reduced: 207390.8549382716, - prehistory\n",
      "Starting professional_accounting, dev size:(5, 6), Test size:(282, 6)\n",
      "Average accuracy 0.521 , Average Time:1.660 sec, avg expert load reduced: 213617.8439716312, - professional_accounting\n",
      "Starting professional_law, dev size:(5, 6), Test size:(1534, 6)\n",
      "Average accuracy 0.501 , Average Time:2.612 sec, avg expert load reduced: 232425.22816166884, - professional_law\n",
      "Starting professional_medicine, dev size:(5, 6), Test size:(272, 6)\n",
      "Average accuracy 0.757 , Average Time:2.105 sec, avg expert load reduced: 251132.3786764706, - professional_medicine\n",
      "Starting professional_psychology, dev size:(5, 6), Test size:(612, 6)\n",
      "Average accuracy 0.714 , Average Time:1.519 sec, avg expert load reduced: 260092.3251633987, - professional_psychology\n",
      "Starting public_relations, dev size:(5, 6), Test size:(110, 6)\n",
      "Average accuracy 0.691 , Average Time:1.345 sec, avg expert load reduced: 267388.1, - public_relations\n",
      "Starting security_studies, dev size:(5, 6), Test size:(245, 6)\n",
      "Average accuracy 0.710 , Average Time:2.141 sec, avg expert load reduced: 270843.6693877551, - security_studies\n",
      "Starting sociology, dev size:(5, 6), Test size:(201, 6)\n",
      "Average accuracy 0.866 , Average Time:1.371 sec, avg expert load reduced: 275160.74626865675, - sociology\n",
      "Starting us_foreign_policy, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.910 , Average Time:1.356 sec, avg expert load reduced: 278121.2, - us_foreign_policy\n",
      "Starting virology, dev size:(5, 6), Test size:(166, 6)\n",
      "Average accuracy 0.518 , Average Time:1.281 sec, avg expert load reduced: 280783.6987951807, - virology\n",
      "Starting world_religions, dev size:(5, 6), Test size:(171, 6)\n",
      "Average accuracy 0.865 , Average Time:1.205 sec, avg expert load reduced: 284182.51461988303, - world_religions\n",
      "Average accuracy 0.429 - math\n",
      "Average accuracy 0.704 - health\n",
      "Average accuracy 0.597 - physics\n",
      "Average accuracy 0.817 - business\n",
      "Average accuracy 0.802 - biology\n",
      "Average accuracy 0.505 - chemistry\n",
      "Average accuracy 0.633 - computer science\n",
      "Average accuracy 0.663 - economics\n",
      "Average accuracy 0.593 - engineering\n",
      "Average accuracy 0.588 - philosophy\n",
      "Average accuracy 0.726 - other\n",
      "Average accuracy 0.789 - history\n",
      "Average accuracy 0.813 - geography\n",
      "Average accuracy 0.804 - politics\n",
      "Average accuracy 0.780 - psychology\n",
      "Average accuracy 0.819 - culture\n",
      "Average accuracy 0.539 - law\n",
      "Average accuracy 0.564 - STEM\n",
      "Average accuracy 0.610 - humanities\n",
      "Average accuracy 0.763 - social sciences\n",
      "Average accuracy 0.727 - other (business, health, misc.)\n",
      "Average accuracy: 0.661\n"
     ]
    }
   ],
   "source": [
    "from mmlu import test_mmlu\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "test_mmlu(model_name=model_name, model_loaded=model, tokenizer=tokenizer, data_dir=\"./mmlu\", save_dir=\"./thresholding_0_15_6E_Off_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result Ref: https://wandb.ai/byyoung3/ml-news/reports/Testing-Mixtral-8x7B-with-MMLU-and-W-B---Vmlldzo2MjI0ODAz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/585587/moe_offload/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Loading experts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  6.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_model(\n",
    "    device=device,\n",
    "    quant_config=quant_config,\n",
    "    offload_config=offload_config,\n",
    "    state_path=state_path,\n",
    "    routing_strategy=\"THRESHOLDING\",\n",
    "    routing_threshold=0.075\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting abstract_algebra, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.340 , Average Time:1.154 sec, avg expert load reduced: 599.33, - abstract_algebra\n",
      "Starting anatomy, dev size:(5, 6), Test size:(135, 6)\n",
      "Average accuracy 0.593 , Average Time:1.153 sec, avg expert load reduced: 1882.8592592592593, - anatomy\n",
      "Starting astronomy, dev size:(5, 6), Test size:(152, 6)\n",
      "Average accuracy 0.789 , Average Time:1.394 sec, avg expert load reduced: 3385.125, - astronomy\n",
      "Starting business_ethics, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.670 , Average Time:1.376 sec, avg expert load reduced: 4655.49, - business_ethics\n",
      "Starting clinical_knowledge, dev size:(5, 6), Test size:(265, 6)\n",
      "Average accuracy 0.758 , Average Time:1.227 sec, avg expert load reduced: 6464.0037735849055, - clinical_knowledge\n",
      "Starting college_biology, dev size:(5, 6), Test size:(144, 6)\n",
      "Average accuracy 0.764 , Average Time:1.286 sec, avg expert load reduced: 8545.583333333334, - college_biology\n",
      "Starting college_chemistry, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.480 , Average Time:1.351 sec, avg expert load reduced: 9841.8, - college_chemistry\n",
      "Starting college_computer_science, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.540 , Average Time:1.601 sec, avg expert load reduced: 10911.43, - college_computer_science\n",
      "Starting college_mathematics, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.390 , Average Time:1.380 sec, avg expert load reduced: 11974.73, - college_mathematics\n",
      "Starting college_medicine, dev size:(5, 6), Test size:(173, 6)\n",
      "Average accuracy 0.665 , Average Time:1.319 sec, avg expert load reduced: 13425.71098265896, - college_medicine\n",
      "Starting college_physics, dev size:(5, 6), Test size:(102, 6)\n",
      "Average accuracy 0.480 , Average Time:1.279 sec, avg expert load reduced: 14842.362745098038, - college_physics\n",
      "Starting computer_security, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.720 , Average Time:1.156 sec, avg expert load reduced: 15900.2, - computer_security\n",
      "Starting conceptual_physics, dev size:(5, 6), Test size:(235, 6)\n",
      "Average accuracy 0.643 , Average Time:1.071 sec, avg expert load reduced: 17734.73191489362, - conceptual_physics\n",
      "Starting econometrics, dev size:(5, 6), Test size:(114, 6)\n",
      "Average accuracy 0.456 , Average Time:1.409 sec, avg expert load reduced: 19684.36842105263, - econometrics\n",
      "Starting electrical_engineering, dev size:(5, 6), Test size:(145, 6)\n",
      "Average accuracy 0.593 , Average Time:1.227 sec, avg expert load reduced: 21048.344827586207, - electrical_engineering\n",
      "Starting elementary_mathematics, dev size:(5, 6), Test size:(378, 6)\n",
      "Average accuracy 0.458 , Average Time:1.336 sec, avg expert load reduced: 23772.53439153439, - elementary_mathematics\n",
      "Starting formal_logic, dev size:(5, 6), Test size:(126, 6)\n",
      "Average accuracy 0.516 , Average Time:1.426 sec, avg expert load reduced: 26448.73015873016, - formal_logic\n",
      "Starting global_facts, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.370 , Average Time:1.211 sec, avg expert load reduced: 27662.6, - global_facts\n",
      "Starting high_school_biology, dev size:(5, 6), Test size:(310, 6)\n",
      "Average accuracy 0.819 , Average Time:1.321 sec, avg expert load reduced: 29839.60322580645, - high_school_biology\n",
      "Starting high_school_chemistry, dev size:(5, 6), Test size:(203, 6)\n",
      "Average accuracy 0.517 , Average Time:1.301 sec, avg expert load reduced: 32580.935960591134, - high_school_chemistry\n",
      "Starting high_school_computer_science, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.770 , Average Time:1.679 sec, avg expert load reduced: 34170.55, - high_school_computer_science\n",
      "Starting high_school_european_history, dev size:(5, 6), Test size:(165, 6)\n",
      "Average accuracy 0.758 , Average Time:2.468 sec, avg expert load reduced: 35511.07878787879, - high_school_european_history\n",
      "Starting high_school_geography, dev size:(5, 6), Test size:(198, 6)\n",
      "Average accuracy 0.813 , Average Time:1.160 sec, avg expert load reduced: 37325.32828282828, - high_school_geography\n",
      "Starting high_school_government_and_politics, dev size:(5, 6), Test size:(193, 6)\n",
      "Average accuracy 0.933 , Average Time:1.250 sec, avg expert load reduced: 39283.678756476686, - high_school_government_and_politics\n",
      "Starting high_school_macroeconomics, dev size:(5, 6), Test size:(390, 6)\n",
      "Average accuracy 0.672 , Average Time:1.157 sec, avg expert load reduced: 42194.015384615384, - high_school_macroeconomics\n",
      "Starting high_school_mathematics, dev size:(5, 6), Test size:(270, 6)\n",
      "Average accuracy 0.315 , Average Time:1.312 sec, avg expert load reduced: 45503.32962962963, - high_school_mathematics\n",
      "Starting high_school_microeconomics, dev size:(5, 6), Test size:(238, 6)\n",
      "Average accuracy 0.748 , Average Time:1.174 sec, avg expert load reduced: 48164.6512605042, - high_school_microeconomics\n",
      "Starting high_school_physics, dev size:(5, 6), Test size:(151, 6)\n",
      "Average accuracy 0.411 , Average Time:1.347 sec, avg expert load reduced: 50177.30463576159, - high_school_physics\n",
      "Starting high_school_psychology, dev size:(5, 6), Test size:(545, 6)\n",
      "Average accuracy 0.855 , Average Time:1.291 sec, avg expert load reduced: 53805.7504587156, - high_school_psychology\n",
      "Starting high_school_statistics, dev size:(5, 6), Test size:(216, 6)\n",
      "Average accuracy 0.579 , Average Time:1.609 sec, avg expert load reduced: 57760.75925925926, - high_school_statistics\n",
      "Starting high_school_us_history, dev size:(5, 6), Test size:(204, 6)\n",
      "Average accuracy 0.828 , Average Time:2.490 sec, avg expert load reduced: 59878.936274509804, - high_school_us_history\n",
      "Starting high_school_world_history, dev size:(5, 6), Test size:(237, 6)\n",
      "Average accuracy 0.819 , Average Time:2.240 sec, avg expert load reduced: 61998.10548523207, - high_school_world_history\n",
      "Starting human_aging, dev size:(5, 6), Test size:(223, 6)\n",
      "Average accuracy 0.700 , Average Time:1.092 sec, avg expert load reduced: 64366.29596412556, - human_aging\n",
      "Starting human_sexuality, dev size:(5, 6), Test size:(131, 6)\n",
      "Average accuracy 0.748 , Average Time:1.131 sec, avg expert load reduced: 66218.29770992366, - human_sexuality\n",
      "Starting international_law, dev size:(5, 6), Test size:(121, 6)\n",
      "Average accuracy 0.818 , Average Time:1.436 sec, avg expert load reduced: 67526.17355371901, - international_law\n",
      "Starting jurisprudence, dev size:(5, 6), Test size:(108, 6)\n",
      "Average accuracy 0.769 , Average Time:1.179 sec, avg expert load reduced: 68766.31481481482, - jurisprudence\n",
      "Starting logical_fallacies, dev size:(5, 6), Test size:(163, 6)\n",
      "Average accuracy 0.761 , Average Time:1.219 sec, avg expert load reduced: 70162.69938650307, - logical_fallacies\n",
      "Starting machine_learning, dev size:(5, 6), Test size:(112, 6)\n",
      "Average accuracy 0.518 , Average Time:1.446 sec, avg expert load reduced: 71575.99107142857, - machine_learning\n",
      "Starting management, dev size:(5, 6), Test size:(103, 6)\n",
      "Average accuracy 0.786 , Average Time:1.062 sec, avg expert load reduced: 72715.7572815534, - management\n",
      "Starting marketing, dev size:(5, 6), Test size:(234, 6)\n",
      "Average accuracy 0.893 , Average Time:1.193 sec, avg expert load reduced: 74498.70512820513, - marketing\n",
      "Starting medical_genetics, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.750 , Average Time:1.139 sec, avg expert load reduced: 76275.95, - medical_genetics\n",
      "Starting miscellaneous, dev size:(5, 6), Test size:(783, 6)\n",
      "Average accuracy 0.845 , Average Time:1.092 sec, avg expert load reduced: 80810.83269476373, - miscellaneous\n",
      "Starting moral_disputes, dev size:(5, 6), Test size:(346, 6)\n",
      "Average accuracy 0.728 , Average Time:1.271 sec, avg expert load reduced: 86648.80635838151, - moral_disputes\n",
      "Starting moral_scenarios, dev size:(5, 6), Test size:(895, 6)\n",
      "Average accuracy 0.412 , Average Time:1.449 sec, avg expert load reduced: 93716.85363128492, - moral_scenarios\n",
      "Starting nutrition, dev size:(5, 6), Test size:(306, 6)\n",
      "Average accuracy 0.771 , Average Time:1.400 sec, avg expert load reduced: 100324.46078431372, - nutrition\n",
      "Starting philosophy, dev size:(5, 6), Test size:(311, 6)\n",
      "Average accuracy 0.727 , Average Time:1.113 sec, avg expert load reduced: 103525.31832797428, - philosophy\n",
      "Starting prehistory, dev size:(5, 6), Test size:(324, 6)\n",
      "Average accuracy 0.759 , Average Time:1.323 sec, avg expert load reduced: 106870.41975308642, - prehistory\n",
      "Starting professional_accounting, dev size:(5, 6), Test size:(282, 6)\n",
      "Average accuracy 0.521 , Average Time:1.499 sec, avg expert load reduced: 110083.92907801419, - professional_accounting\n",
      "Starting professional_law, dev size:(5, 6), Test size:(1534, 6)\n",
      "Average accuracy 0.501 , Average Time:2.444 sec, avg expert load reduced: 119364.43350717079, - professional_law\n",
      "Starting professional_medicine, dev size:(5, 6), Test size:(272, 6)\n",
      "Average accuracy 0.757 , Average Time:1.943 sec, avg expert load reduced: 128583.79779411765, - professional_medicine\n",
      "Starting professional_psychology, dev size:(5, 6), Test size:(612, 6)\n",
      "Average accuracy 0.714 , Average Time:1.364 sec, avg expert load reduced: 133058.27614379086, - professional_psychology\n",
      "Starting public_relations, dev size:(5, 6), Test size:(110, 6)\n",
      "Average accuracy 0.691 , Average Time:1.189 sec, avg expert load reduced: 136668.57272727272, - public_relations\n",
      "Starting security_studies, dev size:(5, 6), Test size:(245, 6)\n",
      "Average accuracy 0.710 , Average Time:1.978 sec, avg expert load reduced: 138410.77142857143, - security_studies\n",
      "Starting sociology, dev size:(5, 6), Test size:(201, 6)\n",
      "Average accuracy 0.866 , Average Time:1.215 sec, avg expert load reduced: 140621.62189054728, - sociology\n",
      "Starting us_foreign_policy, dev size:(5, 6), Test size:(100, 6)\n",
      "Average accuracy 0.910 , Average Time:1.200 sec, avg expert load reduced: 142223.03, - us_foreign_policy\n",
      "Starting virology, dev size:(5, 6), Test size:(166, 6)\n",
      "Average accuracy 0.518 , Average Time:1.128 sec, avg expert load reduced: 143635.98795180724, - virology\n",
      "Starting world_religions, dev size:(5, 6), Test size:(171, 6)\n",
      "Average accuracy 0.865 , Average Time:1.053 sec, avg expert load reduced: 145378.58479532163, - world_religions\n",
      "Average accuracy 0.429 - math\n",
      "Average accuracy 0.704 - health\n",
      "Average accuracy 0.597 - physics\n",
      "Average accuracy 0.817 - business\n",
      "Average accuracy 0.802 - biology\n",
      "Average accuracy 0.505 - chemistry\n",
      "Average accuracy 0.633 - computer science\n",
      "Average accuracy 0.663 - economics\n",
      "Average accuracy 0.593 - engineering\n",
      "Average accuracy 0.588 - philosophy\n",
      "Average accuracy 0.726 - other\n",
      "Average accuracy 0.789 - history\n",
      "Average accuracy 0.813 - geography\n",
      "Average accuracy 0.804 - politics\n",
      "Average accuracy 0.780 - psychology\n",
      "Average accuracy 0.819 - culture\n",
      "Average accuracy 0.539 - law\n",
      "Average accuracy 0.564 - STEM\n",
      "Average accuracy 0.610 - humanities\n",
      "Average accuracy 0.763 - social sciences\n",
      "Average accuracy 0.727 - other (business, health, misc.)\n",
      "Average accuracy: 0.661\n"
     ]
    }
   ],
   "source": [
    "from mmlu import test_mmlu\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "test_mmlu(model_name=model_name, model_loaded=model, tokenizer=tokenizer, data_dir=\"./mmlu\", save_dir=\"./thresholding_0_075_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
