{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix numpy in colab\n",
    "import numpy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# fix triton in colab\n",
    "!export LC_ALL=\"en_US.UTF-8\"\n",
    "!export LD_LIBRARY_PATH=\"/etc/alternatives/cuda/targets/x86_64-linux/include:/usr/include/python3.6m:$LD_LIBRARY_PATH\"\n",
    "!export LIBRARY_PATH=\"/etc/alternatives/cuda/lib64/stubs\"\n",
    "# !ldconfig /etc/alternatives/cuda/lib64/lib64-nvidia\n",
    "\n",
    "# !git clone https://github.com/dvmazur/mixtral-offloading.git --quiet\n",
    "# !cd mixtral-offloading && pip install -q -r requirements.txt\n",
    "!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "script_dir = os.getcwd()\n",
    "module_path = script_dir\n",
    "for _ in range(1):\n",
    "    module_path = os.path.abspath(os.path.join(module_path, '../'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.insert(0,module_path)\n",
    "        \n",
    "sys.path.append(\"mixtral-offloading\")\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from hqq.core.quantize import BaseQuantizeConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import trange\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers.utils import logging as hf_logging\n",
    "import time\n",
    "from src.build_model import OffloadConfig, QuantConfig, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will reload the imported modules (e.g. get_decode_model_characterstics) every time you execute the jupyter cells, so that you don't need to restart the notebook after updating the source codes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading experts: 100%|██████████| 32/32 [00:10<00:00,  3.02it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(quantized_model_name)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n",
    "offload_per_layer = 4\n",
    "# offload_per_layer = 5\n",
    "###############################################################\n",
    "\n",
    "num_experts = config.num_local_experts\n",
    "\n",
    "offload_config = OffloadConfig(\n",
    "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
    "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
    "    buffer_size=4,\n",
    "    offload_per_layer=offload_per_layer,\n",
    ")\n",
    "\n",
    "\n",
    "attn_config = BaseQuantizeConfig(\n",
    "    nbits=4,\n",
    "    group_size=64,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
    "\n",
    "\n",
    "ffn_config = BaseQuantizeConfig(\n",
    "    nbits=2,\n",
    "    group_size=16,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
    "\n",
    "\n",
    "model = build_model(\n",
    "    device=device,\n",
    "    quant_config=quant_config,\n",
    "    offload_config=offload_config,\n",
    "    state_path=state_path,\n",
    "    routing_strategy=\"THRESHOLDING\",\n",
    "    routing_threshold=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: \n",
      "\n",
      "Mixtral: \n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "past_key_values = None\n",
    "sequence = None\n",
    "\n",
    "seq_len = 0\n",
    "# while True:\n",
    "print(\"User: \", end=\"\")\n",
    "user_input = \"Where is Georgia Tech? What is the name of its mascot?\"\n",
    "print(\"\\n\")\n",
    "\n",
    "user_entry = dict(role=\"user\", content=user_input)\n",
    "input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n",
    "\n",
    "if past_key_values is None:\n",
    "  attention_mask = torch.ones_like(input_ids)\n",
    "else:\n",
    "  seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n",
    "  attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n",
    "\n",
    "print(\"Mixtral: \\n\", end=\"\")\n",
    "\n",
    "\n",
    "# sequence = result[\"sequences\"]\n",
    "# past_key_values = result[\"past_key_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFAULT with 4 Experts on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Georgia Tech, officially known as the Georgia Institute of Technology, is located in Atlanta, Georgia, in the United States. The institution is part of the University System of Georgia and is renowned for its strong programs in engineering, computing, and the sciences.\n",
      "\n",
      "The Georgia Tech Yellow Jackets' mascot is a yellow jacket, specifically a modified version of a hornet. The mascot is often depicted in a stylized, cartoon form with a black and gold color scheme, reflecting the school's colors. The nickname for the mascot is \"Buzz,\" and the school's official mascot is known as the \"Georgia Tech Yellow Jacket.\"\n",
      "\n",
      "The school's mascot is a source of pride for the student body, and the yellow jacket is a symbol that is used extensively in school spirit items, clothing, and merchandise. Georgia Tech's mascot is a beloved representation of the\n",
      "Total Latency :92.60623288154602 sec \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "result = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  attention_mask=attention_mask,\n",
    "  past_key_values=past_key_values,\n",
    "  streamer=streamer,\n",
    "  do_sample=True,\n",
    "  temperature=0.9,\n",
    "  top_p=0.9,\n",
    "  min_new_tokens=200,\n",
    "  max_new_tokens=200,\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  return_dict_in_generate=True,\n",
    "  output_hidden_states=True,\n",
    ")\n",
    "latency = time.time() - start_time\n",
    "\n",
    "print(f\"Total Latency :{latency} sec \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding with 4 Experts on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Georgia Tech, officially known as the Georgia Institute of Technology, is a public research university located in Atlanta, Georgia, in the United States. The university is part of the University System of Georgia and is a top-ranking institution in the fields of engineering, computing, and related sciences.\n",
      "\n",
      "The mascot of Georgia Tech is a yellow-colored, English-speaking bird known as the Georgia Tech Yellow Jacket. The mascot is depicted as a fierce, energetic, and determined creature, which is intended to reflect the spirit of the university's students, faculty, and alumni. The Yellow Jacket mascot has been a fixture of Georgia Tech athletics and campus life for many decades, and it continues to be a source of pride and identity for the institution and its community.\n",
      "\n",
      "The Georgia Tech Yellow Jackets are the university's intercollegiate athletic teams, which compete in the National Collegiate Athletic Association (NC\n",
      "Total Latency :81.12173986434937 sec \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "result = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  attention_mask=attention_mask,\n",
    "  past_key_values=past_key_values,\n",
    "  streamer=streamer,\n",
    "  do_sample=True,\n",
    "  temperature=0.9,\n",
    "  top_p=0.9,\n",
    "  min_new_tokens=200,\n",
    "  max_new_tokens=200,\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  return_dict_in_generate=True,\n",
    "  output_hidden_states=True,\n",
    ")\n",
    "latency = time.time() - start_time\n",
    "\n",
    "print(f\"Total Latency :{latency} sec \")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate Perplexity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate.loading import evaluation_module_factory\n",
    "from datasets import DownloadConfig, DownloadMode, Version\n",
    "!pip install -U evaluate\n",
    "from utils.perplexity_local_model import Perplexity\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "\n",
    "def calculate_perplexity(predictions: List[str], model, tokenizer):\n",
    "    config_name: Optional[str] = None\n",
    "    module_type: Optional[str] = None\n",
    "    process_id: int = 0\n",
    "    num_process: int = 1\n",
    "    cache_dir: Optional[str] = None\n",
    "    experiment_id: Optional[str] = None\n",
    "    keep_in_memory: bool = False\n",
    "    download_config: Optional[DownloadConfig] = None\n",
    "    download_mode: Optional[DownloadMode] = None\n",
    "    revision: Optional[Union[str, Version]] = None\n",
    "    \n",
    "    perplexity_module = evaluation_module_factory(\n",
    "        \"perplexity\", module_type=module_type, revision=revision, download_config=download_config, download_mode=download_mode\n",
    "    )\n",
    "    \n",
    "    perplexity = Perplexity(\n",
    "        config_name=config_name,\n",
    "        process_id=process_id,\n",
    "        num_process=num_process,\n",
    "        cache_dir=cache_dir,\n",
    "        keep_in_memory=keep_in_memory,\n",
    "        experiment_id=experiment_id,\n",
    "        hash=perplexity_module.hash\n",
    "    )\n",
    "    \n",
    "    if module_type and module_type != perplexity.module_type:\n",
    "        raise TypeError(\n",
    "            f\"No module of module type '{module_type}' not found for 'perplexity' locally, or on the Hugging Face Hub. Found module of module type '{perplexity.module_type}' instead.\"\n",
    "        )\n",
    "    \n",
    "    # Download and prepare resources for the metric\n",
    "    perplexity.download_and_prepare(download_config=download_config)\n",
    "    \n",
    "    # predictions, model, tokenizer, batch_size: int = 16, add_start_token: bool = True, device=None, max_length=None\n",
    "    \n",
    "    return perplexity.compute(predictions=predictions, add_start_token=False, model=model, tokenizer=tokenizer)['mean_perplexity']\n",
    "\n",
    "calculate_perplexity([\"Hello world is a common programming print statement.\", \"Perplexity is only useful within the same model\"], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# With help from GPT\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Load the C4 dataset\n",
    "dataset = load_dataset(\"c4\", \"en\", split=\"validation\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Evaluation function\n",
    "def evaluate_model_on_dataset(model, tokenizer, dataset):\n",
    "    macro_batch_size = 512\n",
    "    running_avg_pplx = None\n",
    "    n = None\n",
    "    \n",
    "    for batch in dataset.iter(batch_size=macro_batch_size):\n",
    "        texts = batch['text']\n",
    "        next_batch_avg_pplx = calculate_perplexity(texts, model, tokenizer)\n",
    "        if running_avg_pplx is not None:\n",
    "            running_avg_pplx = (running_avg_pplx * n + next_batch_avg_pplx * len(texts)) / (n + len(texts))\n",
    "            n += len(texts)\n",
    "        else:\n",
    "            running_avg_pplx = next_batch_avg_pplx\n",
    "            n = len(texts)\n",
    "    \n",
    "    return running_avg_pplx, n\n",
    "\n",
    "\n",
    "## Get perplexity on C4 using non-instruction fine-tuned model\n",
    "routing_strategies = ['DEFAULT', 'THRESHOLDING', 'BIASING']\n",
    "\n",
    "for routing_strategy in routing_strategies:\n",
    "    dataset = load_dataset(\"c4\", \"en\", split=\"validation\")\n",
    "    model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "    model = build_model(\n",
    "        device=device,\n",
    "        quant_config=quant_config,\n",
    "        offload_config=offload_config,\n",
    "        state_path=state_path,\n",
    "        routing_strategy=routing_strategy,\n",
    "        routing_threshold=0.05,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    mean_pplx, n = evaluate_model_on_dataset(model, tokenizer, dataset)\n",
    "    print(f\"{routing_strategy} on C4 dataset | Avg perplexity: {mean_pplx}, n_samples = {n}\")\n",
    "\n",
    "### Using residency information\n",
    "\n",
    "## Get accuracy on MMLU using instruction fine-tuned model\n",
    "\n",
    "### Using default routing\n",
    "\n",
    "### Using thresholding\n",
    "\n",
    "### Using residency information"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe_offload",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
