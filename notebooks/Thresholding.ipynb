{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  5 20:04:48 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA TITAN RTX               Off |   00000000:1A:00.0 Off |                  N/A |\n",
      "| 38%   42C    P0             66W /  280W |       1MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA TITAN RTX               Off |   00000000:68:00.0 Off |                  N/A |\n",
      "| 35%   44C    P0             68W /  280W |       1MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix numpy in colab\n",
    "import numpy\n",
    "from IPython.display import clear_output\n",
    "import os, sys\n",
    "\n",
    "if not os.path.exists(\"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"):\n",
    "    !huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --cache-dir /tmp --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/abhimanyu/work/Google-MoE/mixtral-offloading-residency-info/notebooks/Mixtral-8x7B-Instruct-v0.1-offloading-full-demo\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"Mixtral-8x7B-Instruct-v0.1\"):\n",
    "    !huggingface-cli download mistralai/Mixtral-8x7B-Instruct-v0.1 --quiet --cache-dir /tmp --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-full-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mhqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhimanyu/miniconda3/envs/moe_offload/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys\n",
    "script_dir = os.getcwd()\n",
    "module_path = script_dir\n",
    "for _ in range(1):\n",
    "    module_path = os.path.abspath(os.path.join(module_path, '../'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.insert(0,module_path)\n",
    "        \n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from hqq.core.quantize import BaseQuantizeConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import trange\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers.utils import logging as hf_logging\n",
    "import time\n",
    "import gc\n",
    "from src.build_model import OffloadConfig, QuantConfig, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will reload the imported modules (e.g. get_decode_model_characterstics) every time you execute the jupyter cells, so that you don't need to restart the notebook after updating the source codes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "if not full:\n",
    "    quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "    state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "    config = AutoConfig.from_pretrained(quantized_model_name)\n",
    "else:\n",
    "    state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-full-demo\"\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n",
    "offload_per_layer = 6\n",
    "# offload_per_layer = 5\n",
    "###############################################################\n",
    "\n",
    "num_experts = config.num_local_experts\n",
    "\n",
    "offload_config = OffloadConfig(\n",
    "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
    "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
    "    buffer_size=2,\n",
    "    offload_per_layer=offload_per_layer,\n",
    ")\n",
    "\n",
    "\n",
    "attn_config = BaseQuantizeConfig(\n",
    "    nbits=4,\n",
    "    group_size=64,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
    "\n",
    "\n",
    "ffn_config = BaseQuantizeConfig(\n",
    "    nbits=2,\n",
    "    group_size=16,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
    "\n",
    "\n",
    "# del model\n",
    "\n",
    "gc.collect\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFAULT with 4 Experts on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhimanyu/miniconda3/envs/moe_offload/lib/python3.11/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 34.81 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 156.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfull\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouting_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTOP-K\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouting_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.10\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/abhimanyu/work/Google-MoE/mixtral-offloading-residency-info/src/build_model.py:233\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(device, quant_config, offload_config, state_path, routing_strategy, routing_threshold, model_name)\u001b[0m\n\u001b[1;32m    231\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(load_file(trunk_state_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(device)), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 233\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_safetensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m expert_cache \u001b[38;5;241m=\u001b[39m ExpertCache(\n\u001b[1;32m    236\u001b[0m     make_module\u001b[38;5;241m=\u001b[39m_make_module,\n\u001b[1;32m    237\u001b[0m     main_size\u001b[38;5;241m=\u001b[39moffload_config\u001b[38;5;241m.\u001b[39mmain_size,\n\u001b[1;32m    238\u001b[0m     offload_size\u001b[38;5;241m=\u001b[39moffload_config\u001b[38;5;241m.\u001b[39moffload_size,\n\u001b[1;32m    239\u001b[0m     buffer_size\u001b[38;5;241m=\u001b[39moffload_config\u001b[38;5;241m.\u001b[39mbuffer_size,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m trange(model_config\u001b[38;5;241m.\u001b[39mnum_hidden_layers, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading experts\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/scratch/abhimanyu/work/Google-MoE/mixtral-offloading-residency-info/src/build_model.py:226\u001b[0m, in \u001b[0;36mbuild_model.<locals>.load_model_from_safetensors\u001b[0;34m(model, state_path)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(device)) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 226\u001b[0m             state_dict[key] \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mget_tensor(key)\n\u001b[1;32m    228\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 34.81 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 156.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if 'model' in locals():\n",
    "    del model\n",
    "model = build_model(\n",
    "    device=device,\n",
    "    quant_config=quant_config if not full else None,\n",
    "    offload_config=offload_config,\n",
    "    state_path=state_path,\n",
    "    routing_strategy=\"TOP-K\",\n",
    "    routing_threshold=0.10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "past_key_values = None\n",
    "sequence = None\n",
    "\n",
    "seq_len = 0\n",
    "# while True:\n",
    "user_input = \"Where is Georgia Tech? What is the name of its mascot?\"\n",
    "\n",
    "user_entry = dict(role=\"user\", content=user_input)\n",
    "input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "if past_key_values is None:\n",
    "  attention_mask = torch.ones_like(input_ids)\n",
    "else:\n",
    "  seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n",
    "  attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=model.device)\n",
    "\n",
    "\n",
    "\n",
    "# sequence = result[\"sequences\"]\n",
    "# past_key_values = result[\"past_key_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "result = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  attention_mask=attention_mask,\n",
    "  past_key_values=past_key_values,\n",
    "  streamer=streamer,\n",
    "  do_sample=True,\n",
    "  temperature=0.9,\n",
    "  top_p=0.9,\n",
    "  min_new_tokens=20,\n",
    "  max_new_tokens=20,\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  return_dict_in_generate=True,\n",
    "  output_hidden_states=False,\n",
    "  output_router_logits=False,\n",
    "  output_scores=True, \n",
    ")\n",
    "latency = time.time() - start_time\n",
    "# total_experts_saved = 0\n",
    "# for i in result['router_logits'][-32:]:\n",
    "#     total_experts_saved += i[1]\n",
    "# total_experts_saved \n",
    "\n",
    "Num_tokens = result['sequences'].shape[1] - input_ids.shape[1]\n",
    "print(f\"Total Latency :{latency} sec, Throughput:{Num_tokens/latency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe_offload",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
