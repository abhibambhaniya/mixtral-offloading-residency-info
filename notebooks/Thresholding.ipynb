{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 16 00:25:46 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              72W / 700W |      4MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix numpy in colab\n",
    "import numpy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --cache-dir $TMP_DIR --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "script_dir = os.getcwd()\n",
    "module_path = script_dir\n",
    "for _ in range(1):\n",
    "    module_path = os.path.abspath(os.path.join(module_path, '../'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.insert(0,module_path)\n",
    "        \n",
    "sys.path.append(\"mixtral-offloading\")\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from hqq.core.quantize import BaseQuantizeConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import trange\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers.utils import logging as hf_logging\n",
    "import time\n",
    "import gc\n",
    "from src.build_model import OffloadConfig, QuantConfig, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will reload the imported modules (e.g. get_decode_model_characterstics) every time you execute the jupyter cells, so that you don't need to restart the notebook after updating the source codes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo1\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(quantized_model_name)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n",
    "offload_per_layer = 4\n",
    "# offload_per_layer = 5\n",
    "###############################################################\n",
    "\n",
    "num_experts = config.num_local_experts\n",
    "\n",
    "offload_config = OffloadConfig(\n",
    "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
    "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
    "    buffer_size=4,\n",
    "    offload_per_layer=offload_per_layer,\n",
    ")\n",
    "\n",
    "\n",
    "attn_config = BaseQuantizeConfig(\n",
    "    nbits=4,\n",
    "    group_size=64,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
    "\n",
    "\n",
    "ffn_config = BaseQuantizeConfig(\n",
    "    nbits=2,\n",
    "    group_size=16,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
    "\n",
    "\n",
    "# del model\n",
    "\n",
    "gc.collect\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFAULT with 4 Experts on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/592241/moe_offload/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Loading experts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  6.67it/s]\n"
     ]
    }
   ],
   "source": [
    "if 'model' in locals():\n",
    "    del model\n",
    "model = build_model(\n",
    "    device=device,\n",
    "    quant_config=quant_config,\n",
    "    offload_config=offload_config,\n",
    "    state_path=state_path,\n",
    "    routing_strategy=\"BIASING\",\n",
    "    routing_threshold=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "past_key_values = None\n",
    "sequence = None\n",
    "\n",
    "seq_len = 0\n",
    "# while True:\n",
    "user_input = \"Where is Georgia Tech? What is the name of its mascot?\"\n",
    "\n",
    "user_entry = dict(role=\"user\", content=user_input)\n",
    "input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "if past_key_values is None:\n",
    "  attention_mask = torch.ones_like(input_ids)\n",
    "else:\n",
    "  seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n",
    "  attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=model.device)\n",
    "\n",
    "\n",
    "\n",
    "# sequence = result[\"sequences\"]\n",
    "# past_key_values = result[\"past_key_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Georgia Tech, officially known as the Georgia Institute of Technology, is a public research university located in Atlanta, Georgia, in the southeastern United States. It was established in 1885 and has grown to become a leading institution in technology, engineering, and related sciences.\n",
      "\n",
      "The mascot of Georgia Tech is called \"Buzz.\" This mascot is a yellow jacket, which is also the nickname and official mascot of Georgia Tech's sports teams.\n",
      "Total Latency :18.324641704559326 sec, Throughput:5.457132620231214, Expert Loads saved:926 \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "result = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  attention_mask=attention_mask,\n",
    "  past_key_values=past_key_values,\n",
    "  streamer=streamer,\n",
    "  do_sample=True,\n",
    "  temperature=0.9,\n",
    "  top_p=0.9,\n",
    "  min_new_tokens=100,\n",
    "  max_new_tokens=100,\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  return_dict_in_generate=True,\n",
    "  # output_hidden_states=True,\n",
    "  # decoder_router_logits=True, \n",
    "  output_router_logits=True,\n",
    ")\n",
    "latency = time.time() - start_time\n",
    "total_experts_saved = 0\n",
    "for i in result['router_logits'][-32:]:\n",
    "    total_experts_saved += i[1]\n",
    "total_experts_saved \n",
    "\n",
    "Num_tokens = result['sequences'].shape[1] - input_ids.shape[1]\n",
    "print(f\"Total Latency :{latency} sec, Throughput:{Num_tokens/latency}, Expert Loads saved:{total_experts_saved} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[-1.6494, -0.2023,  0.3101,  1.6348,  0.3503,  0.4883,  0.0604, -0.3401],[-0.8192, -0.8853, -0.8905, -0.9040, -0.8886, -0.8669, -0.8367, -0.9087]])\n",
    "b = torch.tensor([-0.8192, -0.8853, -0.8905, -0.9040, -0.8886, -0.8669, -0.8367, -0.9087])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3512,  0.1791, -0.2761, -1.4779, -0.3113, -0.4233, -0.0505,  0.3090],\n",
       "        [ 0.6711,  0.7838,  0.7930,  0.8172,  0.7896,  0.7515,  0.7001,  0.8257]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_not_in_b(A, B):\n",
    "    return len(set(A.flatten().tolist()) - set(B.flatten().tolist()))\n",
    "\n",
    "# Example usage\n",
    "A = torch.tensor([[1, 5]], device='cuda:0') \n",
    "B = torch.tensor([[2, 7]], device='cuda:0')\n",
    "print(count_not_in_b(A, B))  # Output: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding with 4 Experts on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "result = model.generate(\n",
    "  input_ids=input_ids.to(device),\n",
    "  attention_mask=attention_mask.to(device),\n",
    "  past_key_values=past_key_values,\n",
    "  streamer=streamer,\n",
    "  do_sample=True,\n",
    "  temperature=0.9,\n",
    "  top_p=0.9,\n",
    "  min_new_tokens=200,\n",
    "  max_new_tokens=200,\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  return_dict_in_generate=True,\n",
    "  output_hidden_states=True,\n",
    ")\n",
    "latency = time.time() - start_time\n",
    "\n",
    "print(f\"Total Latency :{latency} sec \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U evaluate\n",
    "!pip install -U datasets\n",
    "\n",
    "from evaluate.loading import evaluation_module_factory\n",
    "from datasets import DownloadConfig, DownloadMode, Version\n",
    "from utils.perplexity_local_model import Perplexity\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "\n",
    "def calculate_perplexity(predictions: List[str], model, tokenizer):\n",
    "    config_name: Optional[str] = None\n",
    "    module_type: Optional[str] = None\n",
    "    process_id: int = 0\n",
    "    num_process: int = 1\n",
    "    cache_dir: Optional[str] = None\n",
    "    experiment_id: Optional[str] = None\n",
    "    keep_in_memory: bool = False\n",
    "    download_config: Optional[DownloadConfig] = None\n",
    "    download_mode: Optional[DownloadMode] = None\n",
    "    revision: Optional[Union[str, Version]] = None\n",
    "    \n",
    "    perplexity_module = evaluation_module_factory(\n",
    "        \"perplexity\", module_type=module_type, revision=revision, download_config=download_config, download_mode=download_mode\n",
    "    )\n",
    "    \n",
    "    perplexity = Perplexity(\n",
    "        config_name=config_name,\n",
    "        process_id=process_id,\n",
    "        num_process=num_process,\n",
    "        cache_dir=cache_dir,\n",
    "        keep_in_memory=keep_in_memory,\n",
    "        experiment_id=experiment_id,\n",
    "        hash=perplexity_module.hash\n",
    "    )\n",
    "    \n",
    "    if module_type and module_type != perplexity.module_type:\n",
    "        raise TypeError(\n",
    "            f\"No module of module type '{module_type}' not found for 'perplexity' locally, or on the Hugging Face Hub. Found module of module type '{perplexity.module_type}' instead.\"\n",
    "        )\n",
    "    \n",
    "    # Download and prepare resources for the metric\n",
    "    perplexity.download_and_prepare(download_config=download_config)\n",
    "    \n",
    "    # predictions, model, tokenizer, batch_size: int = 16, add_start_token: bool = True, device=None, max_length=None\n",
    "    \n",
    "    return perplexity.compute(predictions=predictions, add_start_token=False, model=model, tokenizer=tokenizer)['mean_perplexity']\n",
    "\n",
    "calculate_perplexity([\"Hello world is a common programming print statement.\", \"Perplexity is only useful within the same model\"], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# With help from GPT\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Load the C4 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Evaluation function\n",
    "def evaluate_model_on_dataset(model, tokenizer, dataset):\n",
    "    macro_batch_size = 512\n",
    "    running_avg_pplx = None\n",
    "    n = None\n",
    "    \n",
    "    for batch in dataset.iter(batch_size=macro_batch_size):\n",
    "        texts = batch['text']\n",
    "        next_batch_avg_pplx = calculate_perplexity(texts, model, tokenizer)\n",
    "        if running_avg_pplx is not None:\n",
    "            running_avg_pplx = (running_avg_pplx * n + next_batch_avg_pplx * len(texts)) / (n + len(texts))\n",
    "            n += len(texts)\n",
    "        else:\n",
    "            running_avg_pplx = next_batch_avg_pplx\n",
    "            n = len(texts)\n",
    "    \n",
    "    return running_avg_pplx, n\n",
    "\n",
    "\n",
    "## Get perplexity on C4 using non-instruction fine-tuned model\n",
    "routing_strategies = ['DEFAULT', 'THRESHOLDING', 'BIASING']\n",
    "\n",
    "for routing_strategy in routing_strategies:\n",
    "    dataset = load_dataset(\"datablations/c4-subsets\", split=\"validation\")\n",
    "    model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "    model = build_model(\n",
    "        device=device,\n",
    "        quant_config=quant_config,\n",
    "        offload_config=offload_config,\n",
    "        state_path=state_path,\n",
    "        routing_strategy=routing_strategy,\n",
    "        routing_threshold=0.05,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    mean_pplx, n = evaluate_model_on_dataset(model, tokenizer, dataset)\n",
    "    print(f\"{routing_strategy} on C4 dataset | Avg perplexity: {mean_pplx}, n_samples = {n}\")\n",
    "\n",
    "### Using residency information\n",
    "\n",
    "## Get accuracy on MMLU using instruction fine-tuned model\n",
    "\n",
    "### Using default routing\n",
    "\n",
    "### Using thresholding\n",
    "\n",
    "### Using residency information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
